[["index.html", "Online appendix for the paper: “On the Assessment of Benchmark Suites for Algorithm Comparison” Chapter 1 Foreword 1.1 Session info", " Online appendix for the paper: “On the Assessment of Benchmark Suites for Algorithm Comparison” David Issa Mattos, Lucas Ruud, Jan Bosch and Helena Holmström Olsson. 2021-04-15 Chapter 1 Foreword This is the online appendix for the paper “On the Assessment of Benchmark Suites for Algorithm Comparison”. It contains a commented and reproducible code for all the analysis, tables and plots presented in the paper and additional content. The code used to generate this appendix is available at the repository: https://github.com/davidissamattos/evo-irt 1.1 Session info This appendix is compiled automatically and the following session information was used to generate this appendix: sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] cmdstanr_0.3.0.9000 latex2exp_0.5.0 kableExtra_1.2.1 ## [4] bayesplot_1.8.0 posterior_0.1.3 knitr_1.30 ## [7] forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 ## [10] purrr_0.3.4 readr_1.3.1 tidyr_1.1.2 ## [13] tibble_3.0.4 ggplot2_3.3.3 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.1.0 xfun_0.20 haven_2.3.1 colorspace_2.0-0 ## [5] vctrs_0.3.6 generics_0.1.0 viridisLite_0.3.0 htmltools_0.5.1 ## [9] yaml_2.2.1 blob_1.2.1 rlang_0.4.10 pillar_1.4.7 ## [13] glue_1.4.2 withr_2.3.0 DBI_1.1.0 dbplyr_1.4.4 ## [17] modelr_0.1.8 readxl_1.3.1 plyr_1.8.6 lifecycle_0.2.0 ## [21] munsell_0.5.0 gtable_0.3.0 cellranger_1.1.0 rvest_0.3.6 ## [25] evaluate_0.14 fansi_0.4.1 broom_0.7.0 Rcpp_1.0.5 ## [29] scales_1.1.1 backports_1.2.1 checkmate_2.0.0 webshot_0.5.2 ## [33] jsonlite_1.7.2 abind_1.4-5 fs_1.5.0 hms_0.5.3 ## [37] digest_0.6.27 stringi_1.5.3 bookdown_0.21 grid_4.0.3 ## [41] cli_2.2.0 tools_4.0.3 magrittr_2.0.1 crayon_1.3.4 ## [45] pkgconfig_2.0.3 ellipsis_0.3.1 xml2_1.3.2 ggridges_0.5.3 ## [49] reprex_0.3.0 lubridate_1.7.9 assertthat_0.2.1 rmarkdown_2.6 ## [53] httr_1.4.2 rstudioapi_0.13 R6_2.5.0 compiler_4.0.3 "],["importing-the-data.html", "Chapter 2 Importing the data 2.1 PBO dataset 2.2 BBOB 2019 2.3 BBOB 2009", " Chapter 2 Importing the data In this chapter, we will import the data using the package IOHAnalyzer and the data available in the repositories below. https://github.com/IOHprofiler/IOHdata for the PBO dataset https://numbbo.github.io/data-archive/bbob/ for the BBOB 2019 and 2009 The processed data from this chapter is available in the folder data of the repository https://github.com/davidissamattos/evo-irt library(IOHanalyzer) To import this data, we use the function below to convert to csv format for easier analysis. This gives a summarized values of the number of runs and the number of succeeded runs. import_data &lt;- function(location, folder=F){ #dsl is a datasetlist from IOHanalyzer if(folder==T) dsl &lt;- DataSetList(location) else dsl&lt;- readRDS(location) #each dataset is an element of this list, e.g. dsl[[1]] #each dataset is a list with three items RT (runtime value for fixed target) FV (function value for fixed budget) and PAR (parameters) ds_list_fv &lt;- lapply(dsl,get_FV_overview) ds_list_rt &lt;- lapply(dsl,get_RT_overview) fv &lt;- do.call(rbind,ds_list_fv) rt &lt;- do.call(rbind,ds_list_rt) out &lt;- list(rt=rt, fv=fv) } 2.1 PBO dataset The IOHData repository already has some benchmarks in the RDS format which we will use here. d_pbo &lt;- import_data(&#39;data/IOHdata-master/iohprofiler/2019gecco-ins11-1run.rds&#39;, folder = F) d_pbo_fv &lt;- d_pbo$fv d_pbo_fv$succ &lt;- d_pbo_fv$succ %&gt;% replace_na(0) write_csv(d_pbo_fv,&#39;data/pbo.csv&#39;) 2.2 BBOB 2019 Import folder d_bbob1 &lt;- import_data(&#39;data/BBOB/Adaptive-Two-Mode_bbob_Bodner/&#39;, folder = T) d_bbob2 &lt;- import_data(&#39;data/BBOB/adapt-Nelder-Mead-scipy-2019_bbob_Varelas_Dahito/&#39;, folder = T) d_bbob3 &lt;- import_data(&#39;data/BBOB/BFGS-scipy-2019_bbob_Varelas_Dahito/minimize_on_bbob_budget100000xD/&#39;, folder = T) d_bbob4 &lt;- import_data(&#39;data/BBOB/CG-scipy-2019_bbob_Varelas_Dahito/&#39;, folder = T) d_bbob5 &lt;- import_data(&#39;data/BBOB/COBYLA-scipy-2019_bbob_Varelas_Dahito/&#39;, folder = T) d_bbob6 &lt;- import_data(&#39;data/BBOB/DE-scipy-2019_bbob_Varelas_Dahito/&#39;, folder = T) d_bbob7 &lt;- import_data(&#39;data/BBOB/L-BFGS-B-scipy-2019_bbob_Varelas_Dahito/&#39;, folder = T) d_bbob8 &lt;- import_data(&#39;data/BBOB/Nelder-Mead-scipy-2019_bbob_Varelas_Dahito/&#39;, folder = T) d_bbob9 &lt;- import_data(&#39;data/BBOB/Powell-scipy-2019_bbob_Varelas_Dahito/&#39;, folder = T) d_bbob10 &lt;- import_data(&#39;data/BBOB/RS-4-initIn0_bbob_Brockhoff_Hansen/&#39;, folder = T) d_bbob11 &lt;- import_data(&#39;data/BBOB/RS-5-initIn0_bbob_Brockhoff_Hansen/&#39;, folder = T) d_bbob12 &lt;- import_data(&#39;data/BBOB/RS-6-initIn0_bbob_Brockhoff_Hansen/&#39;, folder = T) d_bbob13 &lt;- import_data(&#39;data/BBOB/TNC-scipy-2019_bbob_Varelas_Dahito/&#39;, folder = T) Now we merge these datasets into a single one d_bbob &lt;- rbind(d_bbob1$fv, d_bbob2$fv, d_bbob3$fv, d_bbob4$fv, d_bbob5$fv, d_bbob6$fv, d_bbob7$fv, d_bbob8$fv, d_bbob9$fv, d_bbob10$fv, d_bbob11$fv, d_bbob12$fv, d_bbob13$fv ) #NA in succ will become 0 d_bbob$succ &lt;- d_bbob$succ %&gt;% replace_na(0) write_csv(d_bbob,&#39;data/bbob2019.csv&#39;) 2.3 BBOB 2009 bbob_2009_dirs &lt;- list.dirs(path = &quot;data/BBOB2009&quot;, full.names = TRUE, recursive = F) bbob2009&lt;-NULL for(i in seq(1:length(bbob_2009_dirs))){ d &lt;- import_data(bbob_2009_dirs[i], folder = T) bbob2009 &lt;- rbind(bbob2009,d$fv) } bbob2009$succ &lt;- bbob2009$succ %&gt;% replace_na(0) write_csv(bbob2009,&#39;data/bbob2009.csv&#39;) "],["irt-stan.html", "Chapter 3 IRT Stan 3.1 2PL model 3.2 Bayesian 2PL with multiple attempts", " Chapter 3 IRT Stan 3.1 2PL model The description and interpretation of the model is available in the paper. Here we are just generating the plots in the paper Function to generate the item characteristic curve. icc_2pl &lt;- function(a, b, label, thetamin=-5, thetamax=5, step=0.1){ x&lt;-seq(from=thetamin, to=thetamax, by=step) y&lt;-exp(a*(x-b))/(1+exp(a*(x-b))) out&lt;-data.frame( Prob=y, Ability=x, label=rep(label,length(x))) return(out) } 3.1.1 Impact of item difficulty Let’s plot three item characteristic curves with different difficulties and the same discrimination. First, we generate the curves: icc_easy &lt;- icc_2pl(a=3,b=-3, label = &#39;Easy (b=-3)&#39;) icc_medium &lt;- icc_2pl(a=3,b=0, label = &#39;Medium (b=0)&#39;) icc_hard &lt;- icc_2pl(a=3,b=3, label = &#39;Hard (b=3)&#39;) icc_b &lt;- rbind(icc_easy,icc_medium,icc_hard) Then we plot it ggplot(icc_b, aes(x=Ability, y=Prob, color=label))+ geom_line()+ labs( title=&#39;Item Characteristic Curve&#39;, # subtitle = &#39;Impact of difficulty parameter&#39;, x=unname(TeX(&quot;Ability ($\\\\theta$)&quot;)), y=&#39;Probability of solving&#39;, color = &#39;Difficulty&#39; )+ theme_bw()+ theme(legend.position = &#39;bottom&#39;) 3.1.2 Impact of item discrimination Let’s plot three item characteristic curves with different discrimination and the same difficulty First, we generate the curves: icc_lowdisc &lt;- icc_2pl(a=0.3,b=0, label = &#39;Low (a=0.3)&#39;) icc_mediumdisc &lt;- icc_2pl(a=1,b=0, label = &#39;Medium (a=1)&#39;) icc_highdisc &lt;- icc_2pl(a=3,b=0, label = &#39;High (a=3)&#39;) icc_a &lt;- rbind(icc_lowdisc,icc_mediumdisc,icc_highdisc) Then we plot them: ggplot(icc_a, aes(x=Ability, y=Prob, color=label))+ geom_line()+ labs( title=&#39;Item Characteristic Curve&#39;, x=unname(TeX(&quot;Ability ($\\\\theta$)&quot;)), y=&#39;Probability of solving&#39;, color = &#39;Discrimination&#39; )+ theme_bw()+ theme(legend.position = &#39;bottom&#39;) 3.1.3 Item Information We can calculate the item information for the 2PL. We use the direct formula for the information from the paper. item_info &lt;- function(a,b,thetamin=-5, thetamax=5,step=0.1){ theta &lt;- seq(from=thetamin, to=thetamax, by=step) p&lt;-exp(a*(theta-b))/(1+exp(a*(theta-b))) q&lt;-1-p info &lt;- a^2*p*q return(list(info=info,theta=theta)) } Plotting this for a few different item characteristic curves (different values of a and b). First we create a long data frame with one column for the information d_item_info_pars &lt;- data.frame(a=c(0.3,1,1.2,0.8,0.6), b=c(1.5,-2.2,1,-0.4,0.9)) d_item_info&lt;-NULL for(row in 1:nrow(d_item_info_pars)){ info_list &lt;- item_info(d_item_info_pars$a[row],d_item_info_pars$b[row]) d_item_info&lt;-rbind( d_item_info, data.frame( information=info_list$info, theta=info_list$theta, label=rep( glue::glue(&#39;a=&#39;,d_item_info_pars$a[row], &#39;,b=&#39;,d_item_info_pars$b[row]), length(info_list$info) ) ) ) } Now we can plot: ggplot(d_item_info, aes(x=theta, y=information, color=label))+ geom_line()+ labs( title=&#39;Item Information Curve&#39;, x=unname(TeX(&quot;Ability ($\\\\theta$)&quot;)), y=&#39;Information&#39;, color = &#39;Item&#39; )+ theme_bw()+ theme(legend.position = &#39;bottom&#39;)+ guides(color=guide_legend(nrow=2,byrow=TRUE)) 3.1.4 Test information For a test, we can calculate the test information as the sum of the individual items. First let’s pivot wider so we can sum the columns: d_item_info_wider &lt;- d_item_info %&gt;% pivot_wider(names_from = &#39;label&#39;, values_from = &#39;information&#39;) %&gt;% mutate(TestInfo = dplyr::select(. ,starts_with(&#39;a=&#39;)) %&gt;% rowSums()) Now we can plot the test information curve: ggplot(d_item_info_wider, aes(x=theta, y=TestInfo)) + geom_line(aes(y=TestInfo))+ labs( title=&#39;Test Information Curve&#39;, x=unname(TeX(&quot;Ability ($\\\\theta$)&quot;)), y=&#39;Information&#39; )+ theme_bw() 3.2 Bayesian 2PL with multiple attempts The Bayesian two parameter logistic item response theory model with multiple attempts is represented by the following equations. \\[ y_{i,p} \\sim \\text{Binomial}(N_{i,p},\\mu_{i,p}) \\\\ \\mu_{i,p} = \\dfrac{\\exp(a_i*(b_i-\\theta_p))}{1+ \\exp(a_i*(b_i-\\theta_p))} \\\\ a_i \\sim \\text{Half-}\\mathcal{N}(0,3) \\\\ b_i \\sim \\mathcal{N}(0,3)\\\\ \\theta_p \\sim \\mathcal{N}(0,3) \\] The Stan code for this model is shown below: show_stan_code(&#39;models/irt2pl.stan&#39;) ## // IRT 2PL with multiple tries ## // Author: David Issa Mattos ## // Date: 5 April 2021 ## ## data { ## int&lt;lower=0&gt; N; // size of the vector ## int&lt;lower=0&gt; y_succ[N]; // number of successful tries ## int&lt;lower=0&gt; N_tries[N]; // number of tries ## int p[N]; // test taker index(the model) ## int&lt;lower=0&gt; Np; // number of test takes (number of models) ## int item[N]; // item index of the test (the dataset) ## int&lt;lower=0&gt; Nitem; // number of items in the test ## } ## ## ## parameters { ## real b[Nitem]; // difficulty parameter ## real&lt;lower=0&gt; a[Nitem]; // discrimination parameter ## real theta[Np]; // ability of the test taker ## } ## ## model { ## real prob[N]; ## ## //Weakly informative priors ## b ~ normal(0, 3); ## a ~ normal(0,3); ## theta ~ normal(0,3); ## ## //Linear gaussian model ## for(i in 1:N){ ## real mu; ## mu = a[item[i]]*(theta[p[i]]- b[item[i]]); ## prob[i] = exp(mu)/(1+exp(mu)); ## } ## y_succ ~ binomial(N_tries,prob); ## ## } ## ## generated quantities{ ## vector[N] log_lik; ## vector[N] y_rep; ## for(i in 1:N){ ## real mu; ## real prob; ## mu = a[item[i]]*(theta[p[i]]- b[item[i]]); ## prob = exp(mu)/(1+exp(mu)); ## log_lik[i] = binomial_lpmf(y_succ[i] | N_tries[i], prob ); ## y_rep[i] = binomial_rng(N_tries[i], prob); ## } ## } "],["case-study-i-bbob-2019.html", "Chapter 4 Case Study I: BBOB 2019 4.1 Importing the data 4.2 Preparing the Stan data 4.3 Diagnostics 4.4 Results", " Chapter 4 Case Study I: BBOB 2019 4.1 Importing the data To illustrate and make the analysis, we will the number of dimensions equal to 5 (since the benchmark functions are all scalable). To do an analysis with different dimensions just change the code here d_bbob &lt;- read_csv(&#39;data/bbob2019.csv&#39;) %&gt;% select(algId, DIM, funcId, runs, succ, budget) %&gt;% filter(DIM==5) %&gt;% mutate(algId_index = as.integer(as.factor(algId))) #vector with the names in order benchmarks &lt;- seq(1,24) algorithms &lt;- levels(as.factor(d_bbob$algId)) 4.2 Preparing the Stan data Creating a list for Stan. bbob_standata &lt;- list( N = nrow(d_bbob), y_succ = as.integer(d_bbob$succ), N_tries = as.integer(d_bbob$runs), p = d_bbob$algId_index, Np = as.integer(length(unique(d_bbob$algId_index))), item = as.integer(d_bbob$funcId), Nitem = as.integer(length(unique(d_bbob$funcId))) ) Calling the model with cmdstanr irt2pl &lt;- cmdstan_model(&#39;models/irt2pl.stan&#39;) fit_bbob &lt;- irt2pl$sample( data= bbob_standata, chains = 4, iter_sampling = 4000, parallel_chains = 4, max_treedepth = 15 ) fit_bbob$save_object(file=&#39;fitted/bbob5.RDS&#39;) To load the fitted model (to save time in compiling this document) fit_bbob&lt;-readRDS(&#39;fitted/bbob5.RDS&#39;) 4.3 Diagnostics Getting the draws from the posterior draws_a &lt;- fit_bbob$draws(&#39;a&#39;) draws_b &lt;- fit_bbob$draws(&#39;b&#39;) draws_theta &lt;- fit_bbob$draws(&#39;theta&#39;) 4.3.1 Traceplots mcmc_trace(draws_a) mcmc_trace(draws_b) mcmc_trace(draws_theta) 4.3.2 Rhat and Effective samples fit_bbob$summary(c(&#39;a&#39;,&#39;b&#39;, &#39;theta&#39;)) %&gt;% kable(caption=&#39;Summary values fit of the model, including effective samples and Rhat&#39;, booktabs=T, digits =3, format=&#39;html&#39;) %&gt;% kable_styling() %&gt;% scroll_box() Table 4.1: Summary values fit of the model, including effective samples and Rhat variable mean median sd mad q5 q95 rhat ess_bulk ess_tail a[1] 6.522 6.430 1.424 1.420 4.318 9.009 1.000 14280.165 11093.421 a[2] 4.324 4.250 1.138 1.115 2.598 6.321 1.000 12287.535 9783.007 a[3] 4.020 3.931 1.189 1.173 2.216 6.127 1.000 10109.111 9225.800 a[4] 2.414 2.328 0.859 0.852 1.181 3.964 1.000 9537.022 8863.253 a[5] 0.184 0.179 0.040 0.038 0.127 0.255 1.001 9960.494 11299.490 a[6] 1.692 1.565 0.762 0.714 0.691 3.139 1.000 9466.497 6907.590 a[7] 1.939 1.821 0.789 0.742 0.865 3.392 1.000 8753.252 6591.984 a[8] 4.318 4.244 1.128 1.095 2.607 6.312 1.000 12418.532 10295.961 a[9] 3.290 3.213 1.042 1.031 1.714 5.119 1.000 11748.731 8343.076 a[10] 3.266 3.192 1.056 1.042 1.665 5.118 1.000 9944.351 6922.556 a[11] 2.939 2.848 1.015 0.996 1.424 4.730 1.001 10865.103 7069.033 a[12] 1.685 1.554 0.764 0.726 0.693 3.123 1.000 9145.424 7175.492 a[13] 1.478 1.347 0.670 0.612 0.629 2.763 1.000 10919.706 8940.304 a[14] 1.478 1.343 0.676 0.606 0.641 2.775 1.001 11612.301 9517.923 a[15] 1.473 1.355 0.666 0.613 0.634 2.707 1.000 9236.735 7824.179 a[16] 1.482 1.347 0.670 0.608 0.641 2.758 1.000 9694.299 7818.154 a[17] 1.476 1.345 0.674 0.618 0.628 2.734 1.001 9164.008 8435.066 a[18] 1.484 1.356 0.666 0.609 0.640 2.761 1.000 10599.824 8511.088 a[19] 1.182 1.113 0.457 0.412 0.576 2.044 1.000 11309.124 8818.726 a[20] 1.388 1.275 0.624 0.582 0.598 2.572 1.000 9183.205 7985.132 a[21] 1.023 0.939 0.439 0.379 0.477 1.857 1.000 8203.122 8222.777 a[22] 0.670 0.632 0.247 0.221 0.347 1.115 1.001 7799.858 8398.404 a[23] 1.495 1.355 0.691 0.631 0.630 2.813 1.000 10373.600 8546.457 a[24] 1.469 1.340 0.675 0.608 0.620 2.766 1.000 10191.232 8757.702 b[1] 0.190 0.189 0.526 0.517 -0.675 1.049 1.003 1655.798 3234.633 b[2] 0.421 0.418 0.537 0.529 -0.458 1.301 1.003 1714.585 3269.607 b[3] 0.126 0.120 0.532 0.528 -0.750 0.999 1.003 1662.807 3212.317 b[4] 0.400 0.384 0.582 0.572 -0.532 1.366 1.003 1815.237 3702.849 b[5] -10.606 -10.553 1.755 1.745 -13.618 -7.825 1.000 9271.595 11117.350 b[6] 1.987 1.784 1.170 0.969 0.476 4.159 1.001 4259.516 6617.055 b[7] 0.788 0.734 0.711 0.633 -0.260 1.992 1.002 2240.195 4102.527 b[8] 0.421 0.420 0.536 0.526 -0.463 1.306 1.003 1742.331 3435.551 b[9] 0.678 0.657 0.590 0.561 -0.248 1.668 1.002 2018.667 3758.684 b[10] 0.690 0.667 0.600 0.572 -0.247 1.680 1.002 2031.685 3763.513 b[11] 0.824 0.788 0.651 0.591 -0.148 1.884 1.001 2192.201 3938.860 b[12] 1.993 1.802 1.164 0.998 0.492 4.163 1.001 4568.658 6773.007 b[13] 2.420 2.202 1.295 1.138 0.709 4.876 1.000 5568.265 7220.730 b[14] 2.413 2.204 1.276 1.138 0.713 4.800 1.001 5160.159 6932.234 b[15] 2.419 2.182 1.298 1.123 0.747 4.936 1.001 4644.611 7607.595 b[16] 2.400 2.195 1.266 1.129 0.717 4.822 1.001 5039.876 7317.296 b[17] 2.420 2.195 1.290 1.142 0.715 4.873 1.001 5288.295 7216.751 b[18] 2.401 2.191 1.280 1.132 0.719 4.779 1.001 5412.655 7130.342 b[19] 1.432 1.336 0.880 0.793 0.186 2.976 1.001 3445.220 5449.716 b[20] 2.406 2.178 1.298 1.140 0.714 4.874 1.001 5362.057 6661.986 b[21] 1.239 1.138 0.905 0.829 -0.056 2.847 1.001 3242.422 4953.712 b[22] 0.810 0.734 0.830 0.763 -0.397 2.233 1.002 3062.358 4872.259 b[23] 2.398 2.192 1.286 1.155 0.702 4.841 1.001 5267.369 6601.671 b[24] 2.441 2.218 1.307 1.153 0.721 4.945 1.001 5375.168 7470.779 theta[1] -0.691 -0.690 0.553 0.542 -1.604 0.212 1.003 1792.092 3591.987 theta[2] -0.024 -0.024 0.528 0.519 -0.892 0.838 1.003 1681.458 3361.608 theta[3] -0.255 -0.254 0.532 0.523 -1.131 0.624 1.003 1656.443 3263.409 theta[4] -3.989 -3.911 1.205 1.175 -6.100 -2.172 1.000 7015.079 9333.640 theta[5] -4.003 -3.904 1.230 1.185 -6.193 -2.156 1.000 7087.536 9941.552 theta[6] -3.993 -3.896 1.201 1.171 -6.094 -2.180 1.000 7278.511 9311.373 theta[7] -4.002 -3.912 1.229 1.200 -6.159 -2.152 1.000 7797.899 8536.771 theta[8] -1.284 -1.271 0.634 0.621 -2.345 -0.284 1.002 2255.589 4664.016 theta[9] 0.436 0.433 0.523 0.519 -0.430 1.292 1.003 1642.108 2997.515 theta[10] -0.359 -0.356 0.537 0.529 -1.250 0.518 1.003 1664.464 3065.114 theta[11] -0.359 -0.360 0.538 0.535 -1.243 0.521 1.003 1652.009 3192.851 theta[12] -0.232 -0.235 0.532 0.524 -1.110 0.644 1.003 1663.252 3203.973 theta[13] -4.002 -3.899 1.220 1.195 -6.193 -2.180 1.000 7255.309 8999.452 4.4 Results Let’s get some summary descriptive statistics of the posterior fit_summary_a_b &lt;- fit_bbob$summary(c(&#39;a&#39;,&#39;b&#39;)) fit_summary_a &lt;- fit_bbob$summary(c(&#39;a&#39;)) fit_summary_b &lt;- fit_bbob$summary(c(&#39;b&#39;)) fit_summary_theta &lt;- fit_bbob$summary(c(&#39;theta&#39;)) 4.4.1 Difficulty and discrimination Table for the benchmark functions table_benchmarks &lt;- fit_summary_a_b %&gt;% select(&#39;Benchmark ID&#39;=variable, Median=median, &#39;CI 5%&#39;=q5, &#39;CI 95%&#39;=q95) table_benchmarks$&#39;Benchmark ID&#39;&lt;-rep(benchmarks,2) kable(table_benchmarks, caption=&#39;Summary values of the discrimination and difficulty level parameters for the BBOB benchmarks&#39;, booktabs=T, digits =3, format=&#39;html&#39;, linesep = &quot;&quot;) %&gt;% kable_styling() %&gt;% pack_rows(&quot;Discrimination value (a)&quot;,1,24) %&gt;% pack_rows(&quot;Difficulty level (b)&quot;,25,48) Table 4.2: Summary values of the discrimination and difficulty level parameters for the BBOB benchmarks Benchmark ID Median CI 5% CI 95% Discrimination value (a) 1 6.430 4.318 9.009 2 4.250 2.598 6.321 3 3.931 2.216 6.127 4 2.328 1.181 3.964 5 0.179 0.127 0.255 6 1.565 0.691 3.139 7 1.821 0.865 3.392 8 4.244 2.607 6.312 9 3.213 1.714 5.119 10 3.192 1.665 5.118 11 2.848 1.424 4.730 12 1.554 0.693 3.123 13 1.347 0.629 2.763 14 1.343 0.641 2.775 15 1.355 0.634 2.707 16 1.347 0.641 2.758 17 1.345 0.628 2.734 18 1.356 0.640 2.761 19 1.113 0.576 2.044 20 1.275 0.598 2.572 21 0.939 0.477 1.857 22 0.632 0.347 1.115 23 1.355 0.630 2.813 24 1.340 0.620 2.766 Difficulty level (b) 1 0.189 -0.675 1.049 2 0.418 -0.458 1.301 3 0.120 -0.750 0.999 4 0.384 -0.532 1.366 5 -10.553 -13.618 -7.825 6 1.784 0.476 4.159 7 0.734 -0.260 1.992 8 0.420 -0.463 1.306 9 0.657 -0.248 1.668 10 0.667 -0.247 1.680 11 0.788 -0.148 1.884 12 1.802 0.492 4.163 13 2.202 0.709 4.876 14 2.204 0.713 4.800 15 2.182 0.747 4.936 16 2.195 0.717 4.822 17 2.195 0.715 4.873 18 2.191 0.719 4.779 19 1.336 0.186 2.976 20 2.178 0.714 4.874 21 1.138 -0.056 2.847 22 0.734 -0.397 2.233 23 2.192 0.702 4.841 24 2.218 0.721 4.945 A more visual representation. mcmc_intervals(draws_a) + scale_y_discrete(labels=benchmarks)+ labs(x=&#39;Discrimination parameter (a)&#39;, y=&#39;Benchmark function ID&#39;, title=&#39;Discrimination parameter distribution (BBOB)&#39;) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. mcmc_intervals(draws_b) + scale_y_discrete(labels=benchmarks)+ labs(x=&#39;Difficulty level parameter (b)&#39;, y=&#39;Benchmark function ID&#39;, title=&#39;Difficulty level parameter distribution (BBOB)&#39;) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. 4.4.2 Ability Creating a table: table_algorithms &lt;- fit_summary_theta %&gt;% select(Algorithms=variable, Median=median, &#39;CI 5%&#39;=q5, &#39;CI 95%&#39;=q95) table_algorithms$Algorithms &lt;- algorithms kable(table_algorithms, caption=&#39;Summary values of the ability level of the algorithms (BBOB)&#39;, booktabs=T, digits =3, format=&#39;html&#39;, linesep = &quot;&quot;) %&gt;% kable_styling() Table 4.3: Summary values of the ability level of the algorithms (BBOB) Algorithms Median CI 5% CI 95% adapt-Nelder-Mead-scipy-2019 -0.690 -1.604 0.212 Adaptive_Two_Mode -0.024 -0.892 0.838 BFGS-scipy-2019 -0.254 -1.131 0.624 CG-scipy-2019 -3.911 -6.100 -2.172 COBYLA-scipy-2019 -3.904 -6.193 -2.156 DE-scipy-2019 -3.896 -6.094 -2.180 L-BFGS-B-scipy-2019 -3.912 -6.159 -2.152 Nelder-Mead-scipy-2019 -1.271 -2.345 -0.284 Powell-scipy-2019 0.433 -0.430 1.292 RS-4-initIn0 -0.356 -1.250 0.518 RS-5-initIn0 -0.360 -1.243 0.521 RS-6-initIn0 -0.235 -1.110 0.644 TNC-scipy-2019 -3.899 -6.193 -2.180 A more visual representation. mcmc_intervals(draws_theta) + scale_y_discrete(labels=algorithms)+ labs(x=unname(TeX(&quot;Ability level ($\\\\theta$)&quot;)), y=&#39;Algorithm&#39;, title=&#39;Ability level parameter distribution (BBOB)&#39;) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. 4.4.3 Item information First let’s create a few helper functions to calculate the item information p_info &lt;- function(a,b, theta){ return(exp(a*(theta-b))/(1+exp(a*(theta-b)))) } q_info &lt;- function(a,b, theta){ return(1-p_info(a,b, theta)) } #a and b are a vector of 3 a[1] is lower q05 a[2] is median and a[3] is q95 #return a data frame ready to be plottted item_info_with_intervals &lt;- function(a,b,item, thetamin=-5, thetamax=5,step=0.1){ theta &lt;- seq(from=thetamin, to=thetamax, by=step) info_median &lt;- a[1]^2*p_info(a[1],b[1],theta)*q_info(a[1],b[1],theta) info_lower &lt;- a[2]^2*p_info(a[2],b[2],theta)*q_info(a[2],b[2],theta) info_higher &lt;- a[3]^2*p_info(a[3],b[3],theta)*q_info(a[3],b[3],theta) out&lt;- data.frame(Information= c(info_lower,info_median,info_higher), theta=c(theta,theta,theta), pars=c(rep(&#39;q05&#39;,length(theta)), rep(&#39;median&#39;,length(theta)), rep(&#39;q95&#39;,length(theta))), item=c(rep(item,length(theta)), rep(item,length(theta)), rep(item,length(theta)))) return(out) } Creating a single data frame item_information_df &lt;- NULL for(i in seq(1:length(benchmarks))){ a&lt;-as.matrix(fit_summary_a[i,c(3,6,7)]) b&lt;-as.matrix(fit_summary_b[i,c(3,6,7)]) iinfo &lt;- item_info_with_intervals(a=a,b=b,item = i,thetamin = -7, thetamax = 5) item_information_df &lt;- rbind(item_information_df,iinfo) } Now we can create an information plot for every item item_information_df %&gt;% pivot_wider(names_from = &#39;pars&#39;, values_from = &#39;Information&#39;) %&gt;% ggplot(aes(x=theta))+ geom_line(aes(y=median), color=&#39;black&#39;)+ facet_wrap(~item, ncol=4) + labs(title=&#39;Item information curve (BBOB)&#39;, x=unname(TeX(&quot;Ability ($\\\\theta$)&quot;)), y=&#39;Information&#39;, color=&#39;Information interval&#39;)+ theme_bw() + theme(legend.position = &#39;bottom&#39;) 4.4.4 Test information We can also look at the test information. First, we need to pivot wider so we can sum the items test_information_df &lt;- item_information_df %&gt;% pivot_wider(names_from = &#39;item&#39;, values_from = &#39;Information&#39;) %&gt;% mutate(TestInfo = dplyr::select(., -theta, -pars) %&gt;% rowSums()) %&gt;% dplyr::select(theta, pars, TestInfo) Now that we have calculated the test parameters we can plot the test information First let’s get a horizontal line to show where the algorithms median ability lies alg_median &lt;- fit_summary_theta %&gt;% mutate(Algorithm=algorithms) %&gt;% select(Algorithm, median) test_information_df %&gt;% dplyr::select(theta, pars, TestInfo) %&gt;% pivot_wider(names_from = &#39;pars&#39;, values_from = &#39;TestInfo&#39;) %&gt;% ggplot(aes(x=theta)) + geom_line(aes(y=median))+ geom_vline(data=alg_median, aes(xintercept=median,color=Algorithm),linetype=&#39;dashed&#39;)+ labs( title=&#39;Test Information Curve (BBOB)&#39;, x=unname(TeX(&quot;Ability ($\\\\theta$)&quot;)), y=&#39;Test information&#39;, color=&#39;Algorithm median&#39; )+ theme_bw()+ guides(color=guide_legend(nrow=5,byrow=TRUE))+ theme(legend.position = &#39;bottom&#39;) "],["case-study-ii-pbo.html", "Chapter 5 Case study II: PBO 5.1 Importing the data 5.2 Preparing the Stan data 5.3 Diagnostics 5.4 Results", " Chapter 5 Case study II: PBO 5.1 Importing the data To illustrate and make the analysis we will use 5 as the number of dimensions for the benchmark functions d_pbo &lt;- read_csv(&#39;data/pbo.csv&#39;) %&gt;% select(algId, DIM, funcId, runs, succ, budget) %&gt;% filter(DIM==16) %&gt;% mutate(algId_index = as.integer(as.factor(algId))) #vector with the names in order benchmarks &lt;- seq(1,23) algorithms &lt;- levels(as.factor(d_pbo$algId)) 5.2 Preparing the Stan data pbo_standata &lt;- list( N = nrow(d_pbo), y_succ = as.integer(d_pbo$succ), N_tries = as.integer(d_pbo$runs), p = d_pbo$algId_index, Np = as.integer(length(unique(d_pbo$algId_index))), item = as.integer(d_pbo$funcId), Nitem = as.integer(length(unique(d_pbo$funcId))) ) irt2pl &lt;- cmdstan_model(&#39;models/irt2pl.stan&#39;) fit_pbo &lt;- irt2pl$sample( data= pbo_standata, seed = seed, chains = 4, iter_sampling = 4000, parallel_chains = 4, max_treedepth = 15 ) fit_pbo$save_object(file=&#39;fitted/pbo16.RDS&#39;) To load the fitted model to save time in compiling this document fit_pbo&lt;-readRDS(&#39;fitted/pbo16.RDS&#39;) 5.3 Diagnostics Getting the draws from the posterior draws_a &lt;- fit_pbo$draws(&#39;a&#39;) draws_b &lt;- fit_pbo$draws(&#39;b&#39;) draws_theta &lt;- fit_pbo$draws(&#39;theta&#39;) 5.3.1 Traceplots mcmc_trace(draws_a) mcmc_trace(draws_b) mcmc_trace(draws_theta) 5.3.2 Rhat and Effective samples fit_pbo$summary(c(&#39;a&#39;,&#39;b&#39;, &#39;theta&#39;)) %&gt;% kable(caption=&#39;Summary values fit of the model, including effective samples and Rhat&#39;, booktabs=T, digits =3, format=&#39;html&#39;) %&gt;% kable_styling() %&gt;% scroll_box() Table 5.1: Summary values fit of the model, including effective samples and Rhat variable mean median sd mad q5 q95 rhat ess_bulk ess_tail a[1] 3.934 3.707 1.837 1.837 1.355 7.305 1.000 10482.518 6617.247 a[2] 3.937 3.701 1.817 1.798 1.391 7.285 1.000 11107.081 6472.795 a[3] 2.713 2.411 1.483 1.351 0.885 5.567 1.000 12066.342 9175.795 a[4] 3.464 3.189 1.716 1.665 1.192 6.686 1.000 21378.536 10784.405 a[5] 2.706 2.367 1.511 1.370 0.888 5.625 1.000 10187.985 8808.061 a[6] 3.460 3.155 1.728 1.671 1.196 6.756 1.000 22233.362 11142.743 a[7] 1.777 1.699 0.565 0.532 1.006 2.816 1.000 9235.783 11044.112 a[8] 5.405 5.220 1.727 1.693 2.877 8.540 1.000 12518.009 11842.410 a[9] 4.112 3.928 1.424 1.379 2.097 6.707 1.000 11774.405 10021.930 a[10] 0.062 0.059 0.031 0.029 0.015 0.117 1.000 6929.124 3558.016 a[11] 3.449 3.170 1.692 1.636 1.206 6.663 1.000 20504.043 11361.605 a[12] 2.916 2.611 1.593 1.532 0.900 5.968 1.000 10505.108 7419.611 a[13] 3.449 3.151 1.722 1.662 1.202 6.678 1.000 21554.343 11428.768 a[14] 1.467 1.408 0.419 0.389 0.890 2.249 1.000 9334.328 10855.270 a[15] 2.943 2.645 1.606 1.527 0.912 6.013 1.000 10472.136 7012.017 a[16] 4.981 4.789 2.025 2.068 2.009 8.585 1.000 13988.593 8593.783 a[17] 1.398 1.274 0.661 0.600 0.571 2.647 1.000 10909.801 8282.476 a[18] 0.497 0.484 0.132 0.126 0.306 0.735 1.001 8682.765 9710.728 a[19] 1.138 1.055 0.470 0.421 0.537 2.014 1.000 8857.538 9008.410 a[20] 0.527 0.491 0.189 0.162 0.289 0.888 1.000 11049.366 11073.602 a[21] 0.428 0.403 0.142 0.125 0.245 0.694 1.001 11939.433 11161.840 a[22] 0.622 0.582 0.231 0.209 0.325 1.056 1.000 10627.937 9775.536 a[23] 2.672 2.581 0.837 0.808 1.467 4.192 1.000 11187.964 9926.076 b[1] -0.683 -0.629 0.739 0.669 -1.930 0.389 1.002 1619.587 3916.449 b[2] -0.674 -0.616 0.742 0.672 -1.901 0.397 1.002 1711.981 3375.299 b[3] -1.988 -1.733 1.347 1.120 -4.570 -0.285 1.001 4367.543 6005.187 b[4] -3.430 -3.185 1.747 1.710 -6.657 -1.031 1.001 8847.775 10212.229 b[5] -2.004 -1.759 1.347 1.138 -4.580 -0.277 1.001 4252.917 6468.271 b[6] -3.469 -3.217 1.771 1.698 -6.777 -1.026 1.000 8131.056 10593.452 b[7] 0.634 0.633 0.581 0.589 -0.326 1.573 1.003 1371.617 2689.014 b[8] -0.047 -0.041 0.573 0.588 -0.988 0.885 1.003 1285.212 2596.902 b[9] -0.157 -0.151 0.586 0.598 -1.131 0.795 1.003 1325.181 2864.499 b[10] -3.540 -3.523 2.295 2.220 -7.331 0.158 1.000 13997.424 8994.802 b[11] -3.435 -3.179 1.759 1.698 -6.716 -1.038 1.000 9022.686 10282.626 b[12] -1.500 -1.300 1.158 0.948 -3.673 -0.027 1.001 3084.613 4728.449 b[13] -3.470 -3.233 1.769 1.729 -6.731 -1.010 1.000 7954.683 9611.534 b[14] 1.101 1.099 0.593 0.608 0.127 2.067 1.003 1465.963 3178.756 b[15] -1.486 -1.288 1.154 0.928 -3.671 -0.038 1.001 3176.183 4225.810 b[16] -0.346 -0.331 0.604 0.606 -1.345 0.608 1.002 1349.709 3100.828 b[17] -1.455 -1.308 1.087 0.957 -3.455 0.009 1.001 3207.599 5478.346 b[18] 1.573 1.578 0.709 0.702 0.401 2.738 1.002 2073.487 4520.739 b[19] -0.768 -0.695 0.835 0.786 -2.238 0.472 1.002 2082.773 4329.199 b[20] -3.476 -3.298 1.558 1.481 -6.347 -1.254 1.001 6173.014 8820.348 b[21] -4.062 -3.890 1.653 1.598 -7.018 -1.678 1.001 6832.772 9577.002 b[22] -2.553 -2.388 1.367 1.277 -5.032 -0.633 1.001 4999.038 6668.300 b[23] 0.355 0.360 0.571 0.584 -0.595 1.275 1.003 1314.129 2899.819 theta[1] 1.504 1.496 0.633 0.638 0.464 2.545 1.003 1619.499 3267.244 theta[2] 4.385 4.290 1.187 1.150 2.589 6.494 1.001 5924.706 9161.527 theta[3] 6.048 5.931 1.520 1.503 3.761 8.694 1.000 9604.680 9284.926 theta[4] 6.544 6.432 1.614 1.605 4.080 9.362 1.000 11620.501 10261.762 theta[5] 6.041 5.913 1.526 1.483 3.736 8.721 1.001 9097.508 10472.750 theta[6] 6.060 5.944 1.535 1.500 3.725 8.758 1.000 8938.844 9589.486 theta[7] 1.947 1.939 0.675 0.669 0.847 3.077 1.003 1801.573 3782.787 theta[8] 0.885 0.886 0.587 0.601 -0.083 1.850 1.003 1391.774 3299.372 theta[9] -0.161 -0.155 0.581 0.601 -1.120 0.777 1.003 1303.435 2552.924 theta[10] 0.681 0.679 0.580 0.598 -0.277 1.625 1.003 1363.143 2642.354 theta[11] -0.084 -0.081 0.565 0.583 -1.021 0.833 1.003 1252.784 2728.216 theta[12] 0.416 0.420 0.567 0.580 -0.518 1.337 1.003 1287.660 2742.103 5.4 Results fit_summary_a_b &lt;- fit_pbo$summary(c(&#39;a&#39;,&#39;b&#39;)) fit_summary_a &lt;- fit_pbo$summary(c(&#39;a&#39;)) fit_summary_b &lt;- fit_pbo$summary(c(&#39;b&#39;)) fit_summary_theta &lt;- fit_pbo$summary(c(&#39;theta&#39;)) 5.4.1 Difficulty and discrimination Table for the benchmark functions table_benchmarks &lt;- fit_summary_a_b %&gt;% select(&#39;Benchmark ID&#39;=variable, Median=median, &#39;CI 5%&#39;=q5, &#39;CI 95%&#39;=q95) table_benchmarks$&#39;Benchmark ID&#39;&lt;-rep(benchmarks,2) kable(table_benchmarks, caption=&#39;Summary values of the discrimination and difficulty level parameters for the PBO benchmarks&#39;, booktabs=T, digits =3, format=&#39;html&#39;, linesep = &quot;&quot;) %&gt;% kable_styling() %&gt;% pack_rows(&quot;Discrimination value (a)&quot;,1,23) %&gt;% pack_rows(&quot;Difficulty level (b)&quot;,23,46) Table 5.2: Summary values of the discrimination and difficulty level parameters for the PBO benchmarks Benchmark ID Median CI 5% CI 95% Discrimination value (a) 1 3.707 1.355 7.305 2 3.701 1.391 7.285 3 2.411 0.885 5.567 4 3.189 1.192 6.686 5 2.367 0.888 5.625 6 3.155 1.196 6.756 7 1.699 1.006 2.816 8 5.220 2.877 8.540 9 3.928 2.097 6.707 10 0.059 0.015 0.117 11 3.170 1.206 6.663 12 2.611 0.900 5.968 13 3.151 1.202 6.678 14 1.408 0.890 2.249 15 2.645 0.912 6.013 16 4.789 2.009 8.585 17 1.274 0.571 2.647 18 0.484 0.306 0.735 19 1.055 0.537 2.014 20 0.491 0.289 0.888 21 0.403 0.245 0.694 22 0.582 0.325 1.056 Difficulty level (b) 23 2.581 1.467 4.192 1 -0.629 -1.930 0.389 2 -0.616 -1.901 0.397 3 -1.733 -4.570 -0.285 4 -3.185 -6.657 -1.031 5 -1.759 -4.580 -0.277 6 -3.217 -6.777 -1.026 7 0.633 -0.326 1.573 8 -0.041 -0.988 0.885 9 -0.151 -1.131 0.795 10 -3.523 -7.331 0.158 11 -3.179 -6.716 -1.038 12 -1.300 -3.673 -0.027 13 -3.233 -6.731 -1.010 14 1.099 0.127 2.067 15 -1.288 -3.671 -0.038 16 -0.331 -1.345 0.608 17 -1.308 -3.455 0.009 18 1.578 0.401 2.738 19 -0.695 -2.238 0.472 20 -3.298 -6.347 -1.254 21 -3.890 -7.018 -1.678 22 -2.388 -5.032 -0.633 23 0.360 -0.595 1.275 mcmc_intervals(draws_a) + scale_y_discrete(labels=benchmarks)+ labs(x=&#39;Discrimination parameter (a)&#39;, y=&#39;Benchmark function ID&#39;, title=&#39;Discrimination parameter distribution (PBO)&#39;) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. mcmc_intervals(draws_b) + scale_y_discrete(labels=benchmarks)+ labs(x=&#39;Difficulty level parameter (b)&#39;, y=&#39;Benchmark function ID&#39;, title=&#39;Difficulty level parameter distribution (PBO)&#39;) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. 5.4.2 Ability Creating a table table_algorithms &lt;- fit_summary_theta %&gt;% select(Algorithms=variable, Median=median, &#39;CI 5%&#39;=q5, &#39;CI 95%&#39;=q95) table_algorithms$Algorithms &lt;- algorithms kable(table_algorithms, caption=&#39;Summary values of the ability level of the algorithms (PBO)&#39;, booktabs=T, digits =3, format=&#39;html&#39;, linesep = &quot;&quot;) %&gt;% kable_styling() Table 5.3: Summary values of the ability level of the algorithms (PBO) Algorithms Median CI 5% CI 95% (1+(λ,λ)) GA 1.496 0.464 2.545 (1+1) EA_&gt;0 4.290 2.589 6.494 (1+1) fGA 5.931 3.761 8.694 (1+10) EA_{r/2,2r} 6.432 4.080 9.362 (1+10) EA_&gt;0 5.913 3.736 8.721 (1+10) EA_logNormal 5.944 3.725 8.758 (1+10) EA_normalized 1.939 0.847 3.077 (1+10) EA_var_ctrl 0.886 -0.083 1.850 (30,30) vGA -0.155 -1.120 0.777 EDA 0.679 -0.277 1.625 gHC -0.081 -1.021 0.833 RLS 0.420 -0.518 1.337 mcmc_intervals(draws_theta) + scale_y_discrete(labels=algorithms)+ labs(x=unname(TeX(&quot;Ability level ($\\\\theta$)&quot;)), y=&#39;Algorithm&#39;, title=&#39;Ability level parameter distribution (PBO)&#39;) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. 5.4.3 Item information We will use the same functions from the BBOB case study Creating a single data frame item_information_df &lt;- NULL for(i in seq(1:length(benchmarks))){ a&lt;-as.matrix(fit_summary_a[i,c(3,6,7)]) b&lt;-as.matrix(fit_summary_b[i,c(3,6,7)]) iinfo &lt;- item_info_with_intervals(a=a,b=b,item = i,thetamin = -7, thetamax = 5) item_information_df &lt;- rbind(item_information_df,iinfo) } Now we can create an information plot for every item item_information_df %&gt;% pivot_wider(names_from = &#39;pars&#39;, values_from = &#39;Information&#39;) %&gt;% ggplot(aes(x=theta))+ geom_line(aes(y=median), color=&#39;black&#39;)+ # geom_line(aes(y=q05), color=&#39;red&#39;, linetype=&#39;dashed&#39;)+ # geom_line(aes(y=q95), color=&#39;blue&#39;, linetype=&#39;dashed&#39;)+ facet_wrap(~item, ncol=4) + labs(title=&#39;Item information curve (PBO)&#39;, x=unname(TeX(&quot;Ability ($\\\\theta$)&quot;)), y=&#39;Information&#39;, color=&#39;Information interval&#39;)+ theme_bw() + theme(legend.position = &#39;bottom&#39;) 5.4.4 Test information We can also look at the test information. First, we need to pivot wider so we can sum the items test_information_df &lt;- item_information_df %&gt;% pivot_wider(names_from = &#39;item&#39;, values_from = &#39;Information&#39;) %&gt;% mutate(TestInfo = dplyr::select(., -theta, -pars) %&gt;% rowSums()) %&gt;% dplyr::select(theta, pars, TestInfo) Now that we have calculated the test parameters we can plot the test information First let’s get a horizontal line to show where the algorithms median ability lies alg_median &lt;- fit_summary_theta %&gt;% mutate(Algorithm=algorithms) %&gt;% select(Algorithm, median) test_information_df %&gt;% dplyr::select(theta, pars, TestInfo) %&gt;% pivot_wider(names_from = &#39;pars&#39;, values_from = &#39;TestInfo&#39;) %&gt;% ggplot(aes(x=theta)) + geom_line(aes(y=median))+ geom_vline(data=alg_median, aes(xintercept=median,color=Algorithm),linetype=&#39;dashed&#39;)+ labs( title=&#39;Test Information Curve (PBO)&#39;, x=unname(TeX(&quot;Ability ($\\\\theta$)&quot;)), y=&#39;Test information&#39;, color=&#39;Algorithm median&#39; )+ theme_bw()+ guides(color=guide_legend(nrow=5,byrow=TRUE))+ theme(legend.position = &#39;bottom&#39;) "],["extra-i-bbob-2009-and-2019.html", "Chapter 6 Extra I: BBOB 2009 and 2019 6.1 Importing the data 6.2 Preparing the Stan data 6.3 Diagnostics 6.4 Results", " Chapter 6 Extra I: BBOB 2009 and 2019 Here we have both 2009 and 2019 together in the analysis 6.1 Importing the data To illustrate and make the analysis we will use 5 as the number of dimensions for the benchmark functions d_bbob2009 &lt;- read_csv(&#39;data/bbob2009.csv&#39;) %&gt;% select(algId, DIM, funcId, runs, succ, budget) %&gt;% filter(DIM==5) d_bbob2019 &lt;- read_csv(&#39;data/bbob2019.csv&#39;) %&gt;% select(algId, DIM, funcId, runs, succ, budget) %&gt;% filter(DIM==5) d_bbob &lt;- rbind(d_bbob2009, d_bbob2019) %&gt;% mutate(algId_index = as.integer(as.factor(algId))) #vector with the names in order benchmarks &lt;- seq(1,24) algorithms &lt;- levels(as.factor(d_bbob$algId)) 6.2 Preparing the Stan data bbob_standata &lt;- list( N = nrow(d_bbob), y_succ = as.integer(d_bbob$succ), N_tries = as.integer(d_bbob$runs), p = d_bbob$algId_index, Np = as.integer(length(unique(d_bbob$algId_index))), item = as.integer(d_bbob$funcId), Nitem = as.integer(length(unique(d_bbob$funcId))) ) irt2pl &lt;- cmdstan_model(&#39;models/irt2pl.stan&#39;) fit_bbob &lt;- irt2pl$sample( data= bbob_standata, seed = seed, chains = 4, iter_sampling = 4000, parallel_chains = 4, max_treedepth = 15 ) fit_bbob$save_object(file=&#39;fitted/bbob-2009-2019-5.RDS&#39;) To load the fitted model to save time in compiling this document fit_bbob&lt;-readRDS(&#39;fitted/bbob-2009-2019-5.RDS&#39;) 6.3 Diagnostics Getting the draws from the posterior draws_a &lt;- fit_bbob$draws(&#39;a&#39;) draws_b &lt;- fit_bbob$draws(&#39;b&#39;) draws_theta &lt;- fit_bbob$draws(&#39;theta&#39;) 6.3.1 Traceplots mcmc_trace(draws_a) mcmc_trace(draws_b) mcmc_trace(draws_theta) 6.4 Results fit_summary_a_b &lt;- fit_bbob$summary(c(&#39;a&#39;,&#39;b&#39;)) fit_summary_a &lt;- fit_bbob$summary(c(&#39;a&#39;)) fit_summary_b &lt;- fit_bbob$summary(c(&#39;b&#39;)) fit_summary_theta &lt;- fit_bbob$summary(c(&#39;theta&#39;)) 6.4.1 Difficulty and discrimination Table for the benchmark functions table_benchmarks &lt;- fit_summary_a_b %&gt;% select(&#39;Benchmark ID&#39;=variable, Median=median, &#39;CI 5%&#39;=q5, &#39;CI 95%&#39;=q95) table_benchmarks$&#39;Benchmark ID&#39;&lt;-rep(benchmarks,2) kable(table_benchmarks, caption=&#39;Summary values of the discrimination and difficulty level parameters for the BBOB-2009 benchmarks&#39;, booktabs=T, digits =3, format=&#39;html&#39;, linesep = &quot;&quot;) %&gt;% kable_styling() %&gt;% pack_rows(&quot;Discrimination value (a)&quot;,1,24) %&gt;% pack_rows(&quot;Difficulty level (b)&quot;,25,48) Table 6.1: Summary values of the discrimination and difficulty level parameters for the BBOB-2009 benchmarks Benchmark ID Median CI 5% CI 95% Discrimination value (a) 1 5.316 3.835 7.369 2 1.887 1.201 2.779 3 2.132 1.365 3.182 4 1.446 0.823 2.379 5 0.053 0.033 0.085 6 0.877 0.497 1.483 7 1.117 0.608 1.932 8 2.068 1.344 2.998 9 1.624 0.996 2.463 10 1.541 0.907 2.360 11 1.326 0.755 2.096 12 0.877 0.505 1.480 13 0.801 0.461 1.361 14 0.801 0.464 1.367 15 0.789 0.457 1.310 16 0.805 0.467 1.357 17 0.800 0.472 1.366 18 0.801 0.466 1.364 19 1.786 1.043 2.794 20 0.730 0.436 1.216 21 0.823 0.436 1.529 22 0.476 0.270 1.015 23 0.800 0.469 1.363 24 0.804 0.467 1.359 Difficulty level (b) 1 0.009 -0.621 0.624 2 0.976 0.217 1.865 3 0.569 -0.131 1.302 4 0.858 0.062 1.857 5 6.603 3.765 10.043 6 2.657 1.281 5.048 7 1.666 0.620 3.413 8 0.856 0.134 1.665 9 1.206 0.382 2.234 10 1.293 0.455 2.485 11 1.603 0.649 3.122 12 2.706 1.319 4.982 13 3.032 1.511 5.580 14 2.960 1.447 5.415 15 2.618 1.270 4.863 16 3.013 1.514 5.479 17 3.045 1.509 5.448 18 2.990 1.481 5.465 19 0.805 0.057 1.723 20 3.018 1.525 5.389 21 2.130 0.796 4.427 22 3.423 1.250 6.472 23 3.029 1.502 5.482 24 2.976 1.487 5.450 mcmc_intervals(draws_a) + scale_y_discrete(labels=benchmarks)+ labs(x=&#39;Discrimination parameter (a)&#39;, y=&#39;Benchmark function ID&#39;, title=&#39;Discrimination parameter distribution (BBOB-2009)&#39;) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. mcmc_intervals(draws_b) + scale_y_discrete(labels=benchmarks)+ labs(x=&#39;Difficulty level parameter (b)&#39;, y=&#39;Benchmark function ID&#39;, title=&#39;Difficulty level parameter distribution (BBOB-2009)&#39;) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. 6.4.2 Ability Creating a table table_algorithms &lt;- fit_summary_theta %&gt;% select(Algorithms=variable, Median=median, &#39;CI 5%&#39;=q5, &#39;CI 95%&#39;=q95) table_algorithms$Algorithms &lt;- algorithms kable(table_algorithms, caption=&#39;Summary values of the ability level of the algorithms (BBOB-2009)&#39;, booktabs=T, digits =3, format=&#39;html&#39;, linesep = &quot;&quot;) %&gt;% kable_styling() Table 6.2: Summary values of the ability level of the algorithms (BBOB-2009) Algorithms Median CI 5% CI 95% adapt-Nelder-Mead-scipy-2019 -0.820 -1.641 -0.085 Adaptive_Two_Mode -0.147 -0.797 0.500 ALPS -0.569 -1.242 0.081 AMALGAM -0.540 -1.215 0.110 BAYEDA -0.570 -1.253 0.085 BFGS -5.993 -9.393 -3.547 BFGS-scipy-2019 -0.442 -1.103 0.204 BIPOP-CMA-ES -0.567 -1.257 0.086 Cauchy-EDA -0.561 -1.234 0.095 CG-scipy-2019 -4.277 -6.687 -2.495 CMA-ESPLUSSEL -0.568 -1.251 0.089 COBYLA-scipy-2019 -4.279 -6.793 -2.518 DASA -0.570 -1.246 0.086 DE-PSO -0.331 -0.981 0.298 DE-scipy-2019 -4.263 -6.758 -2.506 EDA-PSO -0.542 -1.223 0.098 FULLNEWUOA 0.104 -0.529 0.723 G3PCX -0.540 -1.224 0.109 GA -0.570 -1.239 0.087 GLOBAL -0.569 -1.246 0.084 iAMALGAM -0.335 -0.991 0.301 IPOP-SEP-CMA-ES -0.514 -1.193 0.129 L-BFGS-B-scipy-2019 -4.272 -6.810 -2.487 LSfminbnd -0.521 -1.197 0.124 LSstep -0.428 -1.086 0.215 MA-LS-CHAIN -0.526 -1.206 0.120 MCS 0.271 -0.365 0.900 NELDER -5.998 -9.444 -3.478 Nelder-Mead-scipy-2019 -1.591 -2.718 -0.663 NELDERDOERR -0.472 -1.137 0.173 NEWUOA -5.996 -9.352 -3.501 ONEFIFTH -0.264 -0.911 0.368 POEMS -0.527 -1.205 0.123 Powell-scipy-2019 0.718 0.055 1.371 PSO -0.512 -1.188 0.135 PSO_Bounds -0.566 -1.247 0.080 RANDOMSEARCH -0.573 -1.252 0.090 Rosenbrock -0.549 -1.229 0.101 RS-4-initIn0 -0.567 -1.251 0.080 RS-5-initIn0 -0.572 -1.242 0.084 RS-6-initIn0 -0.255 -0.902 0.375 TNC-scipy-2019 -4.275 -6.695 -2.522 VNS -0.563 -1.247 0.078 mcmc_intervals(draws_theta) + scale_y_discrete(labels=algorithms)+ labs(x=unname(TeX(&quot;Ability level ($\\\\theta$)&quot;)), y=&#39;Algorithm&#39;, title=&#39;Ability level parameter distribution (BBOB-2009)&#39;) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. 6.4.3 Item information Now we can create an information plot for every item item_information_df &lt;- NULL for(i in seq(1:length(benchmarks))){ a&lt;-as.matrix(fit_summary_a[i,c(3,6,7)]) b&lt;-as.matrix(fit_summary_b[i,c(3,6,7)]) iinfo &lt;- item_info_with_intervals(a=a,b=b,item = i,thetamin = -7, thetamax = 5) item_information_df &lt;- rbind(item_information_df,iinfo) } Now we can create an information plot for every item item_information_df %&gt;% pivot_wider(names_from = &#39;pars&#39;, values_from = &#39;Information&#39;) %&gt;% ggplot(aes(x=theta))+ geom_line(aes(y=median), color=&#39;black&#39;)+ # geom_line(aes(y=q05), color=&#39;red&#39;, linetype=&#39;dashed&#39;)+ # geom_line(aes(y=q95), color=&#39;blue&#39;, linetype=&#39;dashed&#39;)+ facet_wrap(~item, ncol=4) + labs(title=&#39;Item information curve (BBOB-2009)&#39;, x=unname(TeX(&quot;Ability ($\\\\theta$)&quot;)), y=&#39;Information&#39;, color=&#39;Information interval&#39;)+ theme_bw() + theme(legend.position = &#39;bottom&#39;) 6.4.4 Test information We can also look at the test information. First, we need to pivot wider so we can sum the items test_information_df &lt;- item_information_df %&gt;% pivot_wider(names_from = &#39;item&#39;, values_from = &#39;Information&#39;) %&gt;% mutate(TestInfo = dplyr::select(., -theta, -pars) %&gt;% rowSums()) %&gt;% dplyr::select(theta, pars, TestInfo) Now that we have calculated the test parameters we can plot the test information First let’s get a horizontal line to show where the algorithms median ability lies alg_median &lt;- fit_summary_theta %&gt;% mutate(Algorithm=algorithms) %&gt;% select(Algorithm, median) test_information_df %&gt;% dplyr::select(theta, pars, TestInfo) %&gt;% pivot_wider(names_from = &#39;pars&#39;, values_from = &#39;TestInfo&#39;) %&gt;% ggplot(aes(x=theta)) + geom_line(aes(y=median))+ geom_vline(data=alg_median, aes(xintercept=median,color=Algorithm),linetype=&#39;dashed&#39;)+ labs( title=&#39;Test Information Curve (BBOB-2009)&#39;, x=unname(TeX(&quot;Ability ($\\\\theta$)&quot;)), y=&#39;Test information&#39;, color=&#39;Algorithm median&#39; )+ theme_bw()+ guides(color=guide_legend(nrow=8,byrow=TRUE))+ theme(legend.position = &#39;bottom&#39;) "],["extra-ii-pbo-625-dimensions.html", "Chapter 7 Extra II: PBO 625 dimensions 7.1 Importing the data 7.2 Preparing the Stan data 7.3 Diagnostics 7.4 Results", " Chapter 7 Extra II: PBO 625 dimensions 7.1 Importing the data To illustrate and make the analysis we will use 5 as the number of dimensions for the benchmark functions d_pbo &lt;- read_csv(&#39;data/pbo.csv&#39;) %&gt;% select(algId, DIM, funcId, runs, succ, budget) %&gt;% filter(DIM==625) %&gt;% mutate(algId_index = as.integer(as.factor(algId))) #vector with the names in order benchmarks &lt;- seq(1,23) algorithms &lt;- levels(as.factor(d_pbo$algId)) 7.2 Preparing the Stan data pbo_standata &lt;- list( N = nrow(d_pbo), y_succ = as.integer(d_pbo$succ), N_tries = as.integer(d_pbo$runs), p = d_pbo$algId_index, Np = as.integer(length(unique(d_pbo$algId_index))), item = as.integer(d_pbo$funcId), Nitem = as.integer(length(unique(d_pbo$funcId))) ) irt2pl &lt;- cmdstan_model(&#39;models/irt2pl.stan&#39;) fit_pbo &lt;- irt2pl$sample( data= pbo_standata, seed = seed, chains = 4, iter_sampling = 4000, parallel_chains = 4, max_treedepth = 15 ) fit_pbo$save_object(file=&#39;fitted/pbo625.RDS&#39;) To load the fitted model to save time in compiling this document fit_pbo&lt;-readRDS(&#39;fitted/pbo625.RDS&#39;) 7.3 Diagnostics Getting the draws from the posterior draws_a &lt;- fit_pbo$draws(&#39;a&#39;) draws_b &lt;- fit_pbo$draws(&#39;b&#39;) draws_theta &lt;- fit_pbo$draws(&#39;theta&#39;) 7.3.1 Traceplots mcmc_trace(draws_a) mcmc_trace(draws_b) mcmc_trace(draws_theta) 7.4 Results fit_summary_a_b &lt;- fit_pbo$summary(c(&#39;a&#39;,&#39;b&#39;)) fit_summary_a &lt;- fit_pbo$summary(c(&#39;a&#39;)) fit_summary_b &lt;- fit_pbo$summary(c(&#39;b&#39;)) fit_summary_theta &lt;- fit_pbo$summary(c(&#39;theta&#39;)) 7.4.1 Difficulty and discrimination Table for the benchmark functions table_benchmarks &lt;- fit_summary_a_b %&gt;% select(&#39;Benchmark ID&#39;=variable, Median=median, &#39;CI 5%&#39;=q5, &#39;CI 95%&#39;=q95) table_benchmarks$&#39;Benchmark ID&#39;&lt;-rep(benchmarks,2) kable(table_benchmarks, caption=&#39;Summary values of the discrimination and difficulty level parameters for the PBO benchmarks&#39;, booktabs=T, digits =3, format=&#39;html&#39;, linesep = &quot;&quot;) %&gt;% kable_styling() %&gt;% pack_rows(&quot;Discrimination value (a)&quot;,1,23) %&gt;% pack_rows(&quot;Difficulty level (b)&quot;,23,46) Table 7.1: Summary values of the discrimination and difficulty level parameters for the PBO benchmarks Benchmark ID Median CI 5% CI 95% Discrimination value (a) 1 2.919 1.210 6.017 2 4.455 2.635 7.121 3 1.431 0.775 2.507 4 2.936 1.199 6.089 5 2.693 1.115 5.857 6 2.679 1.095 5.898 7 0.425 0.258 0.709 8 1.402 0.875 2.193 9 0.596 0.271 1.050 10 0.160 0.098 0.259 11 4.982 2.801 8.026 12 5.966 3.669 9.031 13 2.267 1.229 3.941 14 0.429 0.261 0.713 15 4.119 2.726 6.184 16 4.124 2.717 6.154 17 1.154 0.507 2.025 18 0.430 0.261 0.714 19 0.474 0.204 0.851 20 0.833 0.506 1.272 21 0.387 0.149 0.698 22 0.355 0.207 0.605 Difficulty level (b) 23 0.507 0.250 0.873 1 -3.133 -4.765 -1.961 2 -1.601 -2.543 -0.650 3 -3.031 -4.616 -1.801 4 -3.133 -4.780 -1.973 5 -3.522 -5.524 -2.213 6 -3.531 -5.666 -2.218 7 5.279 3.149 8.293 8 -1.929 -3.020 -0.898 9 -1.334 -2.901 -0.219 10 6.256 3.759 9.466 11 -2.276 -3.269 -1.288 12 -2.132 -3.109 -1.159 13 -2.599 -3.765 -1.529 14 5.439 3.279 8.445 15 -1.146 -2.068 -0.220 16 -1.142 -2.083 -0.220 17 1.434 0.381 2.766 18 5.402 3.274 8.395 19 1.347 0.176 3.052 20 -0.323 -1.295 0.624 21 -1.083 -2.847 0.136 22 4.730 2.659 7.668 23 2.017 0.748 3.890 mcmc_intervals(draws_a) + scale_y_discrete(labels=benchmarks)+ labs(x=&#39;Discrimination parameter (a)&#39;, y=&#39;Benchmark function ID&#39;, title=&#39;Discrimination parameter distribution (PBO)&#39;) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. mcmc_intervals(draws_b) + scale_y_discrete(labels=benchmarks)+ labs(x=&#39;Difficulty level parameter (b)&#39;, y=&#39;Benchmark function ID&#39;, title=&#39;Difficulty level parameter distribution (PBO)&#39;) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. 7.4.2 Ability Creating a table table_algorithms &lt;- fit_summary_theta %&gt;% select(Algorithms=variable, Median=median, &#39;CI 5%&#39;=q5, &#39;CI 95%&#39;=q95) table_algorithms$Algorithms &lt;- algorithms kable(table_algorithms, caption=&#39;Summary values of the ability level of the algorithms (PBO)&#39;, booktabs=T, digits =3, format=&#39;html&#39;, linesep = &quot;&quot;) %&gt;% kable_styling() Table 7.2: Summary values of the ability level of the algorithms (PBO) Algorithms Median CI 5% CI 95% (1+(λ,λ)) GA -1.371 -2.306 -0.437 (1+1) EA_&gt;0 2.184 1.027 3.383 (1+1) fGA 0.139 -0.831 1.150 (1+10) EA_{r/2,2r} 0.865 -0.177 1.949 (1+10) EA_&gt;0 0.340 -0.634 1.346 (1+10) EA_logNormal 0.330 -0.652 1.345 (1+10) EA_normalized 0.853 -0.177 1.926 (1+10) EA_var_ctrl 0.625 -0.384 1.709 (30,30) vGA -2.402 -3.382 -1.426 EDA -1.643 -2.587 -0.694 gHC -0.629 -1.557 0.302 RLS 0.935 -0.312 2.158 mcmc_intervals(draws_theta) + scale_y_discrete(labels=algorithms)+ labs(x=unname(TeX(&quot;Ability level ($\\\\theta$)&quot;)), y=&#39;Algorithm&#39;, title=&#39;Ability level parameter distribution (PBO)&#39;) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. 7.4.3 Item information We will use the same functions from the BBOB case study Creating a single data frame item_information_df &lt;- NULL for(i in seq(1:length(benchmarks))){ a&lt;-as.matrix(fit_summary_a[i,c(3,6,7)]) b&lt;-as.matrix(fit_summary_b[i,c(3,6,7)]) iinfo &lt;- item_info_with_intervals(a=a,b=b,item = i,thetamin = -7, thetamax = 5) item_information_df &lt;- rbind(item_information_df,iinfo) } Now we can create an information plot for every item item_information_df %&gt;% pivot_wider(names_from = &#39;pars&#39;, values_from = &#39;Information&#39;) %&gt;% ggplot(aes(x=theta))+ geom_line(aes(y=median), color=&#39;black&#39;)+ # geom_line(aes(y=q05), color=&#39;red&#39;, linetype=&#39;dashed&#39;)+ # geom_line(aes(y=q95), color=&#39;blue&#39;, linetype=&#39;dashed&#39;)+ facet_wrap(~item, ncol=4) + labs(title=&#39;Item information curve (PBO)&#39;, x=unname(TeX(&quot;Ability ($\\\\theta$)&quot;)), y=&#39;Information&#39;, color=&#39;Information interval&#39;)+ theme_bw() + theme(legend.position = &#39;bottom&#39;) 7.4.4 Test information We can also look at the test information. First, we need to pivot wider so we can sum the items test_information_df &lt;- item_information_df %&gt;% pivot_wider(names_from = &#39;item&#39;, values_from = &#39;Information&#39;) %&gt;% mutate(TestInfo = dplyr::select(., -theta, -pars) %&gt;% rowSums()) %&gt;% dplyr::select(theta, pars, TestInfo) Now that we have calculated the test parameters we can plot the test information First let’s get a horizontal line to show where the algorithms median ability lies alg_median &lt;- fit_summary_theta %&gt;% mutate(Algorithm=algorithms) %&gt;% select(Algorithm, median) test_information_df %&gt;% dplyr::select(theta, pars, TestInfo) %&gt;% pivot_wider(names_from = &#39;pars&#39;, values_from = &#39;TestInfo&#39;) %&gt;% ggplot(aes(x=theta)) + geom_line(aes(y=median))+ geom_vline(data=alg_median, aes(xintercept=median,color=Algorithm),linetype=&#39;dashed&#39;)+ labs( title=&#39;Test Information Curve (PBO)&#39;, x=unname(TeX(&quot;Ability ($\\\\theta$)&quot;)), y=&#39;Test information&#39;, color=&#39;Algorithm median&#39; )+ theme_bw()+ guides(color=guide_legend(nrow=5,byrow=TRUE))+ theme(legend.position = &#39;bottom&#39;) "]]
